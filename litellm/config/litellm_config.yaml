model_list:
  # OpenAI
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo                          # The `openai/` prefix will call openai.chat.completions.create
      api_key: os.environ/OPENAI_API_KEY
  - model_name: gpt-3.5-turbo-instruct
    litellm_params:
      model: text-completion-openai/gpt-3.5-turbo-instruct # The `text-completion-openai/` prefix will call openai.completions.create
      api_key: os.environ/OPENAI_API_KEY

# Anthropic
  - model_name: claude-3-5-sonnet 
    litellm_params: 
      model: claude-3-5-sonnet-20240620 
      api_key: "os.environ/ANTHROPIC_API_KEY" 
  - model_name: claude-3-haiku
    litellm_params: 
      model: claude-3-haiku-20240307
      api_key: "os.environ/ANTHROPIC_API_KEY"    
  - model_name: claude-sonnet-4
    litellm_params: 
      model: claude-sonnet-4-20250514
      api_key: "os.environ/ANTHROPIC_API_KEY"    

# Gemini
  - model_name: gemini-2.5-flash-lite
    litellm_params:
      model: gemini/gemini-2.5-flash-lite
      api_key: os.environ/GEMINI_API_KEY
  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: os.environ/GEMINI_API_KEY

# Bedrock
  - model_name: bedrock-claude-4-sonnet
    litellm_params:
      model: bedrock/anthropic.claude-sonnet-4-20250514-v1:0
      model_id: "arn:aws:bedrock:us-west-2:606469916935:inference-profile/global.anthropic.claude-sonnet-4-20250514-v1:0"

guardrails:

  # default behavior is "opt-in"
  # if you enable "default on" then it will always be on
  # if you want to enable it per model/team/apikey, that is enterprise!!
  - guardrail_name: "presidio-pii"
    litellm_params:
      guardrail: presidio  
      mode: "pre_call"
      presidio_language: "en"  
      # default_on: true  
  - guardrail_name: "custom-pre-guard"
    litellm_params:
      guardrail: custom_guardrail.myCustomGuardrail
      mode: "pre_call"        
  - guardrail_name: "openai-moderation-pre"
    litellm_params:
      guardrail: openai_moderation
      mode: "pre_call"
      api_key: os.environ/OPENAI_API_KEY  # Optional if already set globally
      model: "omni-moderation-latest"     # Optional, defaults to omni-moderation-latest
      api_base: "https://api.openai.com/v1"  # Optional, defaults to OpenAI API    

  - guardrail_name: model-armor-shield
    litellm_params:
      guardrail: model_armor
      mode: [pre_call, post_call]  # Run on both input and output
      template_id: "litellm-guardrail"  # Required: Your Model Armor template ID
      project_id: "ceposta-solo-testing"    # Your GCP project ID
      location: "us-central1"          # GCP location (default: us-central1)
      credentials: "/app/credentials.json"  # Path to service account key
      mask_request_content: true       # Enable request content masking
      mask_response_content: true      # Enable response content masking
      fail_on_error: true             # Fail request if Model Armor errors (default: true)

  - guardrail_name: "bedrock-pre-guard"
    litellm_params:
      guardrail: bedrock  # supported values: "aporia", "bedrock", "lakera"
      mode: "pre_call"
      guardrailIdentifier: v1qto5owq7gz      # your guardrail ID on bedrock
      guardrailVersion: "DRAFT"              # your guardrail version on bedrock

  - guardrail_name: "tool-permission-guardrail"
    litellm_params:
      guardrail: tool_permission
      mode: "post_call"
      rules:
        - id: "allow_bash"
          tool_name: "Bash"
          decision: "allow"
        - id: "allow_github_mcp"
          tool_name: "mcp__github_*"
          decision: "allow"
        - id: "allow_aws_documentation"
          tool_name: "mcp__aws-documentation_*_documentation"
          decision: "allow"
        - id: "deny_read_commands"
          tool_name: "Read"
          decision: "deny"
      default_action: "deny"  # Fallback when no rule matches: "allow" or "deny"
      on_disallowed_action: "block"  # How to handle disallowed tools: "block" or "rewrite"      


litellm_settings:
  # set logging to verbose to decide whether to see prompts
  set_verbose: false

  # store audit logs to database
  # enable this to see what changes get made to the proxy
  store_audit_logs: false


general_settings:
  master_key: sk-1234 
  database_url: "postgresql://llmproxy:dbpassword9090@db:5432/litellm" 
  store_model_in_db: true
  store_prompts_in_spend_logs: true   
